{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "pd.options.display.max_columns = 100\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 100\n",
    "from itertools import islice\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from DatumBox import DatumBox\n",
    "import re\n",
    "from urlparse import urlparse\n",
    "import seaborn as sns # To have more than seven colors in pie diagram\n",
    "import plotly.plotly as py # For Gantt Chart\n",
    "import plotly.figure_factory as ff # For Gantt Chart\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sreeram_0xb5e/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import urllib2\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from DatumBox import DatumBox\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os.path as osp\n",
    "import os\n",
    "datum_box = DatumBox(\"2a13913dda346761765020c1f66e34f8\")\n",
    "TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "nltk.download('punkt')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "def main_text(url):\n",
    "    file_name = url.replace('/','#')\n",
    "    if len(file_name) <= 255:\n",
    "        #If the file is already there\n",
    "        if osp.isfile(os.path.abspath(os.curdir)+\"/Websites/\"+file_name):\n",
    "            f = open(os.path.abspath(os.curdir)+\"/Websites/\"+file_name,'r')\n",
    "            #text contains the text from a webpage that are stored locally\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "        #If the file isnt there then text is extracted with beautiful soup and stored locally\n",
    "        else:\n",
    "            html = urllib.urlopen(url).read()\n",
    "            soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "            # kill all script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()    # rip it out\n",
    "\n",
    "            # get text\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # break into lines and remove leading and trailing space on each\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            # break multi-headlines into a line each\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            # drop blank lines\n",
    "            text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "            for char in text:\n",
    "                if(not((ord(char) >= 97 and ord(char) <= 122) or (ord(char) >= 65 and ord(char) <= 90))):\n",
    "                    text = text.replace (char,\" \")\n",
    "            f = open(os.path.abspath(os.curdir)+'/Websites/'+file_name,'w')\n",
    "            f.write(text)\n",
    "            f.close()\n",
    "            print \"/Seen First Time/\",\n",
    "    else:\n",
    "        html = urllib.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        print \"/Cant save in file/\",\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "        for char in text:\n",
    "            if(not((ord(char) >= 97 and ord(char) <= 122) or (ord(char) >= 65 and ord(char) <= 90))):\n",
    "                text = text.replace (char,\" \")\n",
    "    return text\n",
    "\n",
    "def openfile(f):\n",
    "    x = []\n",
    "    y = []\n",
    "    finial = []; i=2\n",
    "    csvfile = open(f,'rb')\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    text1='';text2='';text1_main='';text2_main=''\n",
    "    synonyms=None\n",
    "    for row in reader:\n",
    "        synonyms=None\n",
    "        if i==0:\n",
    "            print str(round(cosine_sim(text1,text2),2))+ \"      \" +str(round(cosine_sim(text1_main,text2_main),2))\n",
    "            i=2\n",
    "            text1='';text1_main=''\n",
    "            text2='';text2_main=''\n",
    "        elif i==1:\n",
    "            text1 = main_text(row['url'])\n",
    "            text1_main = text1 + ' '\n",
    "            for j in text1.split(' '):\n",
    "                synonyms = wordnet.synsets(j)\n",
    "                synonyms = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "                for k in synonyms:\n",
    "                    text1_main += k + ' '\n",
    "            i-=1\n",
    "        elif i==2:\n",
    "            text2 = main_text(row['url'])\n",
    "            text2_main = text2 + ' '\n",
    "            for j in text2.split(' '):\n",
    "                synonyms = wordnet.synsets(j)\n",
    "                synonyms = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "                for k in synonyms:\n",
    "                    text2_main += k + ' '\n",
    "            i-=1\n",
    "    print a\n",
    "    csvfile.close()\n",
    "\n",
    "#openfile('/home/chaitanya/Documents/Codes/IR_Vidhya mam/resources/input/10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordnet_similarity(a,b):\n",
    "    url1 = a\n",
    "    url2 = b\n",
    "    text1='';text2='';text1_main='';text2_main=''\n",
    "    text1 = main_text(url1)\n",
    "    text2 = main_text(url2)\n",
    "    word_count = 0;similar = 0\n",
    "    text1_split = text1.split(' ');text2_split = text2.split(' ')\n",
    "    text1_split = [x.upper() for x in text1_split if x]\n",
    "    text2_split = [x.upper() for x in text2_split if x]\n",
    "    similar = 0.0;word_count = 0.0;flag=0\n",
    "    \n",
    "    for j in text1_split:\n",
    "        flag = 0\n",
    "        if j not in text2_split:\n",
    "            synonyms = wordnet.synsets(j)\n",
    "            synonyms = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "            for k in synonyms:\n",
    "                if k in text2_split:\n",
    "                    flag = 1\n",
    "        else:\n",
    "            similar += 1\n",
    "        if flag == 1:\n",
    "            similar += 1\n",
    "        word_count += 1\n",
    "\n",
    "    for j in text2_split:\n",
    "        flag = 0\n",
    "        if j not in text1_split:\n",
    "            synonyms = wordnet.synsets(j)\n",
    "            synonyms = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "            for k in synonyms:\n",
    "                if k in text1_split:\n",
    "                    flag = 1\n",
    "        else:\n",
    "            similar += 1\n",
    "        if flag == 1:\n",
    "            similar += 1\n",
    "        word_count += 1\n",
    "\n",
    "    #text2_main += k + ' '\n",
    "    if word_count == 0:\n",
    "        return 0\n",
    "    return similar/word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disp_data= pd.read_csv('internal_cache.csv')\n",
    "disp_data = disp_data[~disp_data[\"url\"].str.contains(\"google\")]\n",
    "disp_data = disp_data[~disp_data[\"url\"].str.contains(\"home\")]\n",
    "i_range = 100\n",
    "j_range = 130\n",
    "test_sample = disp_data[i_range:j_range]\n",
    "#test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import obo\n",
    "\n",
    "def find_buffer(url):\n",
    "    text_in_all_urls = []\n",
    "    text = main_text(url)\n",
    "    fullwordlist = obo.stripNonAlphaNum(text)\n",
    "    wordlist = obo.removeStopwords(fullwordlist, obo.stopwords)\n",
    "    dictionary = obo.wordListToFreqDict(wordlist)\n",
    "    sorteddict = obo.sortFreqDict(dictionary)\n",
    "    for i in sorteddict[:30]:\n",
    "        text_in_all_urls.append(i[1])\n",
    "    return text_in_all_urls\n",
    "\n",
    "\n",
    "#print find_buffer(\"https://journalistsresource.org/studies/society/social-media/multitasking-social-media-distraction-what-does-research-say\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Now the commmon most reccuring words\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-92a161ce4d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n\\nNow the commmon most reccuring words\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmain_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.w3schools.com/Js/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c252646d0bc5>\u001b[0m in \u001b[0;36mfind_buffer\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext_in_all_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfullwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstripNonAlphaNum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveStopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullwordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "print \"\\n\\nNow the commmon most reccuring words\\n\"\n",
    "main_text = []\n",
    "final = find_buffer(\"https://www.w3schools.com/Js/\")\n",
    "\n",
    "for i in final:\n",
    "    for j in i:\n",
    "        main_text.append(j)\n",
    "main_text = obo.wordListToFreqDict(main_text[:])\n",
    "main_text = obo.sortFreqDict(main_text)\n",
    "for i in main_text[:10]:\n",
    "    print i[1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The', 'Uber'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = []\n",
    "cluster_buffer = []\n",
    "thres = 0.2\n",
    "\n",
    "for i in range(i_range,j_range):\n",
    "    curr_url = disp_data.iloc[i]['url']\n",
    "    \n",
    "    if (len(clusters) == 0):\n",
    "        clusters.append(list(str(temp)))\n",
    "        cluster_buffer.append([])\n",
    "        \n",
    "    if(len(clusters) > 0):\n",
    "        c_indx = 0\n",
    "        max_sim = 0\n",
    "        \n",
    "        for c in range(0,len(clusters)):\n",
    "            if (len(clusters[c]) < 2):\n",
    "                ws = wordnet_similarity(clusters[i],curr_url);\n",
    "            else:\n",
    "                ws = wordnet_similarity_url_text(curr_url,cluster_buffer)\n",
    "\n",
    "            if (ws > max_sim):\n",
    "                    max_sim = ws\n",
    "                    c_indx = c\n",
    "        \n",
    "        if (max_sim > thres):\n",
    "            clusters[c].append(curr_url)\n",
    "            #Cluster buffer case\n",
    "            \n",
    "            #When there are strings in the buffer\n",
    "            if (len(cluster_buffer[i]) > 2):\n",
    "            \n",
    "            #When the buffer is empty\n",
    "            else:\n",
    "            \n",
    "        else:\n",
    "            clusters.append(list(str(curr_url)))\n",
    "            cluster_buffer.append([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
